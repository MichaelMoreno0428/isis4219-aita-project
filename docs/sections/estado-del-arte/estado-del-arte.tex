\section{Estado del arte}

El análisis automatizado de decisiones morales en comunidades en línea, especialmente en Reddit, ha captado la atención académica por su potencial para mejorar la comprensión de las dinámicas sociales en estas plataformas. En particular, investigaciones recientes se han enfocado en estudiar cómo las comunidades virtuales realizan juicios éticos sobre situaciones personales complejas, buscando identificar factores que influyen en el veredicto colectivo, tales como características del contenido, perfil del usuario o señales sociales implícitas.

Botzer \textit{et al.} \cite{botzer2023} abordaron este tema mediante un análisis detallado del subreddit \texttt{r/AmITheAsshole} (AITA), examinando la influencia del género, la edad y el tipo de juicio moral expresado sobre la popularidad y recepción social de las publicaciones. Sus hallazgos mostraron que las historias consideradas moralmente positivas suelen tener mayor aceptación y popularidad, reflejada en una mayor cantidad de votos positivos y comentarios generados por la comunidad. Este estudio utilizó modelos avanzados basados en BERT (Judge-BERT) para clasificar publicaciones según el juicio moral colectivo, destacando la importancia de considerar tanto factores lingüísticos como sociales en la dinámica de aceptación del contenido. Aunque su investigación contribuye significativamente a identificar qué factores influyen en la popularidad de las publicaciones, no considera directamente la automatización de veredictos como herramienta práctica para apoyar a los moderadores, aspecto central en nuestra propuesta.

Por otro lado, Alhassan \textit{et al.} \cite{alhassan2022} presentaron una investigación centrada exclusivamente en la predicción del juicio moral en AITA utilizando modelos avanzados de procesamiento de lenguaje natural (RoBERTa y Longformer). Sus resultados demostraron una alta efectividad (hasta 87\% de precisión), confirmando que estos modelos pueden replicar de forma acertada las decisiones colectivas tomadas por la comunidad. Esta investigación es relevante para nuestro trabajo, ya que valida la aplicación de modelos avanzados para clasificar juicios morales en textos de interacciones cotidianas en línea. Sin embargo, no profundizan en otros aspectos claves que abordaremos, como la influencia de la longitud del texto y otros factores contextuales en la aceptación general de las publicaciones.

Osama y Bsher \cite{osama2023} introdujeron una perspectiva diferente al investigar la generación automática de comentarios con razonamiento moral explícito mediante transformers como BART y T5. Este enfoque mostró que es posible producir contenido automatizado moralmente fundamentado con resultados convincentes desde el punto de vista humano. Aunque nuestro proyecto no contempla la generación de comentarios, este trabajo resalta la complejidad de captar matices éticos mediante técnicas avanzadas de NLP, enfatizando la necesidad de entender profundamente las señales implícitas y explícitas presentes en el contenido generado por los usuarios.

Strathern \textit{et al.} \cite{strathern2020} aportan una perspectiva complementaria al estudiar cómo surgen las "explosiones de indignación moral" (firestorms) en Twitter. Su estudio analizó 21 casos de indignación colectiva, desarrollando métodos para detectar sistemáticamente estos cambios mediante señales lingüísticas. Los autores encontraron patrones significativos, como la disminución en el uso del pronombre ``yo'' y el incremento de negatividad durante estos episodios. Utilizando técnicas de detección de puntos de cambio, lograron identificar el inicio de estas explosiones aproximadamente media hora antes de su manifestación evidente, demostrando que los cambios en el uso del lenguaje en comentarios textuales pueden proporcionar información sobre el comportamiento cambiante y la perspectiva cambiante.

Preniqi \textit{et al.} \cite{preniqi2024} desarrollaron MoralBERT, un modelo de lenguaje especializado para capturar valores morales en discursos sociales basado en la Teoría de los Fundamentos Morales. Utilizando datasets heterogéneos de Twitter, Reddit y Facebook, implementaron tanto entrenamiento agregado como adversarial de dominio para mejorar la generalización entre plataformas. Sus resultados mostraron que el marco propuesto logra una puntuación F1 promedio entre 11\% y 32\% más alta que enfoques tradicionales, y particularmente relevante para nuestro trabajo es su hallazgo de que modelos especializados de tamaño moderado pueden competir con LLMs gigantes para detectar valores morales, sugiriendo que un enfoque similar podría ser efectivo para la clasificación automática de veredictos en AITA sin requerir recursos computacionales excesivos.
